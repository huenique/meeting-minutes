"""
RAG (Retrieval-Augmented Generation) Engine for Meetily.

Combines question detection, document retrieval, and LLM-based answer
generation. Integrates with existing LLM providers (Ollama, Claude, Groq,
OpenAI) via the same infrastructure as the summary pipeline.
"""

import logging
import os
from typing import Dict, List, Optional

from pydantic import BaseModel

from document_indexer import DocumentIndexer
from question_detector import DetectedQuestion, detect_questions

logger = logging.getLogger(__name__)


class RAGContext(BaseModel):
    """Context assembled for RAG-based answer generation."""
    question: str
    meeting_context: str
    retrieved_chunks: List[Dict]


class RAGAnswer(BaseModel):
    """An answer generated by the RAG pipeline."""
    question: str
    answer: str
    sources: List[Dict]
    confidence: float


class QuestionWithAnswer(BaseModel):
    """A detected question paired with its generated answer."""
    question: DetectedQuestion
    answer: Optional[RAGAnswer] = None


class RAGEngine:
    """
    Orchestrates question detection, knowledge retrieval, and answer generation.

    Uses the existing LLM provider infrastructure for answer generation and
    ChromaDB (via DocumentIndexer) for knowledge retrieval.
    """

    def __init__(
        self,
        persist_directory: Optional[str] = None,
        collection_name: str = "meetily_knowledge_base",
    ):
        """
        Initialize the RAG engine.

        Args:
            persist_directory: ChromaDB persistence directory.
            collection_name: ChromaDB collection name.
        """
        self._indexer = DocumentIndexer(
            persist_directory=persist_directory,
            collection_name=collection_name,
        )
        logger.info("RAGEngine initialized")

    @property
    def indexer(self) -> DocumentIndexer:
        """Access the underlying document indexer."""
        return self._indexer

    def detect_and_retrieve(
        self,
        transcript_text: str,
        n_results: int = 5,
    ) -> List[QuestionWithAnswer]:
        """
        Detect questions in a transcript and retrieve relevant context.

        This does NOT generate answers (that requires async LLM calls).
        Use generate_answer() separately for each question.

        Args:
            transcript_text: The meeting transcript text.
            n_results: Number of knowledge base results per question.

        Returns:
            List of QuestionWithAnswer with retrieved sources but no answers yet.
        """
        questions = detect_questions(transcript_text)

        results: List[QuestionWithAnswer] = []
        for q in questions:
            retrieved = self._indexer.search(q.text, n_results=n_results)
            results.append(
                QuestionWithAnswer(
                    question=q,
                    answer=RAGAnswer(
                        question=q.text,
                        answer="",  # To be filled by generate_answer
                        sources=retrieved,
                        confidence=0.0,
                    ),
                )
            )

        logger.info(
            f"Detected {len(questions)} questions, "
            f"retrieved context for each"
        )
        return results

    async def generate_answer(
        self,
        question: str,
        meeting_context: str,
        model: str = "ollama",
        model_name: str = "llama3.2",
        n_results: int = 5,
    ) -> RAGAnswer:
        """
        Generate an answer for a question using RAG.

        Retrieves relevant documents from the knowledge base,
        combines them with the meeting context, and sends to
        the configured LLM for answer generation.

        Args:
            question: The question to answer.
            meeting_context: Recent transcript text for context.
            model: LLM provider ('ollama', 'claude', 'groq', 'openai').
            model_name: Specific model name.
            n_results: Number of knowledge base results to include.

        Returns:
            A RAGAnswer with the generated answer and sources.
        """
        # Retrieve relevant context from knowledge base
        retrieved = self._indexer.search(question, n_results=n_results)

        # Build context string from retrieved documents
        knowledge_context = ""
        if retrieved:
            knowledge_parts = []
            for i, chunk in enumerate(retrieved, 1):
                source = chunk.get("metadata", {}).get("filename", "unknown")
                knowledge_parts.append(
                    f"[Source {i}: {source}]\n{chunk['text']}"
                )
            knowledge_context = "\n\n".join(knowledge_parts)

        # Construct the RAG prompt
        prompt = _build_rag_prompt(question, meeting_context, knowledge_context)

        # Generate answer using the appropriate LLM provider
        answer_text = await _call_llm(prompt, model, model_name)

        confidence = 0.8 if retrieved else 0.4

        return RAGAnswer(
            question=question,
            answer=answer_text,
            sources=[
                {
                    "filename": c.get("metadata", {}).get("filename", ""),
                    "text_preview": c["text"][:200],
                    "distance": c.get("distance"),
                }
                for c in retrieved
            ],
            confidence=confidence,
        )


def _build_rag_prompt(
    question: str,
    meeting_context: str,
    knowledge_context: str,
) -> str:
    """Build the prompt for RAG-based answer generation."""
    parts = [
        "You are a helpful meeting assistant. Answer the following question "
        "based on the meeting context and any relevant knowledge provided.\n"
    ]

    if meeting_context:
        parts.append(
            f"## Meeting Context (recent transcript)\n{meeting_context}\n"
        )

    if knowledge_context:
        parts.append(
            f"## Relevant Knowledge Base Documents\n{knowledge_context}\n"
        )

    parts.append(
        f"## Question\n{question}\n\n"
        "## Instructions\n"
        "- Answer the question concisely and accurately.\n"
        "- Reference specific sources when possible.\n"
        "- If the information is not available in the provided context, "
        "say so clearly.\n"
        "- Focus on being helpful and factual.\n"
    )

    return "\n".join(parts)


async def _call_llm(
    prompt: str,
    model: str,
    model_name: str,
) -> str:
    """
    Call the LLM provider to generate a response.

    Reuses the same provider infrastructure as the transcript processor.
    """
    from db import DatabaseManager

    db = DatabaseManager()

    try:
        if model == "ollama":
            return await _call_ollama(prompt, model_name)
        elif model == "claude":
            api_key = await db.get_api_key("claude")
            if not api_key:
                raise ValueError("Anthropic API key not configured")
            return await _call_pydantic_ai(prompt, model, model_name, api_key)
        elif model == "groq":
            api_key = await db.get_api_key("groq")
            if not api_key:
                raise ValueError("Groq API key not configured")
            return await _call_pydantic_ai(prompt, model, model_name, api_key)
        elif model == "openai":
            api_key = await db.get_api_key("openai")
            if not api_key:
                raise ValueError("OpenAI API key not configured")
            return await _call_pydantic_ai(prompt, model, model_name, api_key)
        else:
            raise ValueError(f"Unsupported model provider: {model}")
    except Exception as e:
        logger.error(f"Error calling LLM ({model}/{model_name}): {e}")
        raise


async def _call_ollama(prompt: str, model_name: str) -> str:
    """Call Ollama for answer generation."""
    from ollama import AsyncClient

    ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
    client = AsyncClient(host=ollama_host)

    try:
        response = await client.chat(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
        )
        return response["message"]["content"]
    except Exception as e:
        logger.error(f"Ollama error: {e}")
        raise


async def _call_pydantic_ai(
    prompt: str,
    model: str,
    model_name: str,
    api_key: str,
) -> str:
    """Call a cloud LLM provider via pydantic-ai."""
    from pydantic_ai import Agent

    if model == "claude":
        from pydantic_ai.models.anthropic import AnthropicModel
        from pydantic_ai.providers.anthropic import AnthropicProvider

        llm = AnthropicModel(
            model_name, provider=AnthropicProvider(api_key=api_key)
        )
    elif model == "groq":
        from pydantic_ai.models.groq import GroqModel
        from pydantic_ai.providers.groq import GroqProvider

        llm = GroqModel(model_name, provider=GroqProvider(api_key=api_key))
    elif model == "openai":
        from pydantic_ai.models.openai import OpenAIModel
        from pydantic_ai.providers.openai import OpenAIProvider

        llm = OpenAIModel(model_name, provider=OpenAIProvider(api_key=api_key))
    else:
        raise ValueError(f"Unsupported provider for pydantic-ai: {model}")

    agent = Agent(llm, result_type=str)
    result = await agent.run(prompt)
    return result.data
